{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dk3156/Dongje-Kim-CS4613/blob/main/hw02/mle_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGn0W--fA7NT"
      },
      "source": [
        "# Gaussian Maximum Likelihood\n",
        "\n",
        "##  MLE of a  Gaussian $p_{model}(x|w)$\n",
        "\n",
        "You are given an array of data points called `data`. Your course site plots the negative log-likelihood  function for several candidate hypotheses. Estimate the parameters of the Gaussian $p_{model}$ by  coding an implementation that estimates its optimal parameters (15 points) and explaining what it does (10 points). You are free to use any Gradient-based optimization method you like.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6t_p16bA7NW",
        "outputId": "ef185c66-3a0d-4270-c60c-0800e1390d78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[6.21426542 5.79616902]\n",
            "6.214285714285714 5.882653061224489\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "data = [4, 5, 7, 8, 8, 9, 10, 5, 2, 3, 5, 4, 8, 9]\n",
        "\n",
        "#derive the partial derivative of the function with respect to mean\n",
        "def part_der_mean(mean, var, array):\n",
        "    sums = []\n",
        "    for elem in array:\n",
        "        sums.append(elem - mean)\n",
        "    sums = sum(sums)\n",
        "    return 1 / var * sums\n",
        "\n",
        "#derive the partial derivative of the function with respect to variance\n",
        "def part_der_var(mean, var, array):\n",
        "    sums = []\n",
        "    for elem in array:\n",
        "        sums.append((elem - mean) ** 2)\n",
        "    sums = sum(sums)\n",
        "    return 1 / (2 * var) * (-len(array) + 1 / var * sums)\n",
        "\n",
        "#input = input - learning rate * gradient \n",
        "p = np.array([0, 1])\n",
        "for i in range(500):\n",
        "    gradient = np.array([-part_der_mean(p[0], p[1], data), -part_der_var(p[0], p[1], data)])\n",
        "    p = p - 0.01 * gradient\n",
        "    \n",
        "print(p)\n",
        "print(np.mean(data), np.var(data))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MgeQSkHA7NX"
      },
      "source": [
        "Explanation:\n",
        "For 500 iterations, update parameters so that it best approximates the mean and variance of the original data set. The approximation is done through gradient descent -- getting a gradient for each iteration and updating parameters in the direction to minimize the loss function. Our loss function is negative log of the maximum likelihood function. \n",
        "\n",
        "part_der_mean and part_der_var functions gives us partial derivative of the parameters to get the gradient. \n",
        "p = p - 0.01 * gradient is the formula for updating the paramters. The learning rate is 0.01.\n",
        "\n",
        "As we can see, the real mean and variance is pretty close to the ones that are approximated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__L_QDy1A7NY"
      },
      "source": [
        "## MLE of a conditional Gaussian $p_{model}(y|x,w)$\n",
        "\n",
        "You are given a problem that involves the relationship between $x$ and $y$. Estimate the parameters of a $p_{model}$ that fit the dataset (x,y) shown below.   You are free to use any Gradient-based optimization method you like.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1iQZRpzA7NY",
        "outputId": "9251cb09-781a-4229-bed5-07396db16c20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nan nan nan]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/mk/p5y6cznx60788rys6_dnljxm0000gn/T/ipykernel_49062/3552888126.py:24: RuntimeWarning: overflow encountered in scalar power\n",
            "  ls.append((y[i] - w * x[i] + beta)**2)\n",
            "/var/folders/mk/p5y6cznx60788rys6_dnljxm0000gn/T/ipykernel_49062/3552888126.py:25: RuntimeWarning: overflow encountered in scalar add\n",
            "  res = sum(ls)\n",
            "/var/folders/mk/p5y6cznx60788rys6_dnljxm0000gn/T/ipykernel_49062/3552888126.py:16: RuntimeWarning: invalid value encountered in scalar add\n",
            "  ls.append(y[i] * (y[i] - (w * x[i] + beta)) * x[i])\n",
            "/var/folders/mk/p5y6cznx60788rys6_dnljxm0000gn/T/ipykernel_49062/3552888126.py:8: RuntimeWarning: invalid value encountered in scalar add\n",
            "  ls.append(y[i] * (y[i] - (w * x[i] + beta)))\n",
            "/var/folders/mk/p5y6cznx60788rys6_dnljxm0000gn/T/ipykernel_49062/3552888126.py:26: RuntimeWarning: invalid value encountered in scalar multiply\n",
            "  res = (1 / var) * res\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def log_likelihood_beta(beta, var, w, x, y): \n",
        "    ls = []\n",
        "    for i in range(len(x)):\n",
        "        ls.append(y[i] * (y[i] - (w * x[i] + beta)))\n",
        "    res = sum(ls)\n",
        "    res = (1 / var) * res\n",
        "    return res\n",
        "\n",
        "def log_likelihood_w(beta, var, w, x, y): \n",
        "    ls = []\n",
        "    for i in range(len(x)):\n",
        "        ls.append(y[i] * (y[i] - (w * x[i] + beta)) * x[i]) \n",
        "    res = sum(ls)\n",
        "    res = (1 / var) * res\n",
        "    return res\n",
        "\n",
        "def log_likelihood_var(beta, var, w, x, y): \n",
        "    ls = []\n",
        "    for i in range(len(x)):\n",
        "        ls.append((y[i] - w * x[i] + beta)**2)\n",
        "    res = sum(ls)\n",
        "    res = (1 / var) * res\n",
        "    res = (1 / ( 2 * var )) * (res - len(x))\n",
        "    return res\n",
        "\n",
        "x = np.array([8, 16, 22, 33, 50, 51])\n",
        "y = np.array([5, 20, 14, 32, 42, 58])\n",
        "w = [1, 1, 1] \n",
        "\n",
        "for i in range(1000):\n",
        "    gradient = np.array([-log_likelihood_w(w[0], w[1],w[2], x,y), -log_likelihood_beta(w[0], w[1],w[2], x,y), -log_likelihood_var(w[0], w[1],w[2], x,y)])\n",
        "    w = w - 0.01 * gradient\n",
        "\n",
        "print(w)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}