{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Gaussian Maximum Likelihood\n","\n","##  MLE of a  Gaussian $p_{model}(x|w)$\n","\n","You are given an array of data points called `data`. Your course site plots the negative log-likelihood  function for several candidate hypotheses. Estimate the parameters of the Gaussian $p_{model}$ by  coding an implementation that estimates its optimal parameters (15 points) and explaining what it does (10 points). You are free to use any Gradient-based optimization method you like.  "]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[6.21426542 5.79616902]\n","6.214285714285714 5.882653061224489\n"]}],"source":["import numpy as np\n","\n","data = [4, 5, 7, 8, 8, 9, 10, 5, 2, 3, 5, 4, 8, 9]\n","\n","#derive the partial derivative of the function with respect to mean\n","def part_der_mean(mean, var, array):\n","    sums = []\n","    for elem in array:\n","        sums.append(elem - mean)\n","    sums = sum(sums)\n","    return 1 / var * sums\n","\n","#derive the partial derivative of the function with respect to variance\n","def part_der_var(mean, var, array):\n","    sums = []\n","    for elem in array:\n","        sums.append((elem - mean) ** 2)\n","    sums = sum(sums)\n","    return 1 / (2 * var) * (-len(array) + 1 / var * sums)\n","\n","#input = input - learning rate * gradient \n","p = np.array([0, 1])\n","for i in range(500):\n","    gradient = np.array([-part_der_mean(p[0], p[1], data), -part_der_var(p[0], p[1], data)])\n","    p = p - 0.01 * gradient\n","    \n","print(p)\n","print(np.mean(data), np.var(data))\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Explanation:\n","For 500 iterations, update parameters so that it best approximates the mean and variance of the original data set. The approximation is done through gradient descent -- getting a gradient for each iteration and updating parameters in the direction to minimize the loss function. Our loss function is negative log of the maximum likelihood function. \n","\n","part_der_mean and part_der_var functions gives us partial derivative of the parameters to get the gradient. \n","p = p - 0.01 * gradient is the formula for updating the paramters. The learning rate is 0.01.\n","\n","As we can see, the real mean and variance is pretty close to the ones that are approximated."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## MLE of a conditional Gaussian $p_{model}(y|x,w)$\n","\n","You are given a problem that involves the relationship between $x$ and $y$. Estimate the parameters of a $p_{model}$ that fit the dataset (x,y) shown below.   You are free to use any Gradient-based optimization method you like.  \n"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[nan nan nan]\n"]},{"name":"stderr","output_type":"stream","text":["/var/folders/mk/p5y6cznx60788rys6_dnljxm0000gn/T/ipykernel_49062/3552888126.py:24: RuntimeWarning: overflow encountered in scalar power\n","  ls.append((y[i] - w * x[i] + beta)**2)\n","/var/folders/mk/p5y6cznx60788rys6_dnljxm0000gn/T/ipykernel_49062/3552888126.py:25: RuntimeWarning: overflow encountered in scalar add\n","  res = sum(ls)\n","/var/folders/mk/p5y6cznx60788rys6_dnljxm0000gn/T/ipykernel_49062/3552888126.py:16: RuntimeWarning: invalid value encountered in scalar add\n","  ls.append(y[i] * (y[i] - (w * x[i] + beta)) * x[i])\n","/var/folders/mk/p5y6cznx60788rys6_dnljxm0000gn/T/ipykernel_49062/3552888126.py:8: RuntimeWarning: invalid value encountered in scalar add\n","  ls.append(y[i] * (y[i] - (w * x[i] + beta)))\n","/var/folders/mk/p5y6cznx60788rys6_dnljxm0000gn/T/ipykernel_49062/3552888126.py:26: RuntimeWarning: invalid value encountered in scalar multiply\n","  res = (1 / var) * res\n"]}],"source":["import numpy as np\n","\n","def log_likelihood_beta(beta, var, w, x, y): \n","    ls = []\n","    for i in range(len(x)):\n","        ls.append(y[i] * (y[i] - (w * x[i] + beta)))\n","    res = sum(ls)\n","    res = (1 / var) * res\n","    return res\n","\n","def log_likelihood_w(beta, var, w, x, y): \n","    ls = []\n","    for i in range(len(x)):\n","        ls.append(y[i] * (y[i] - (w * x[i] + beta)) * x[i]) \n","    res = sum(ls)\n","    res = (1 / var) * res\n","    return res\n","\n","def log_likelihood_var(beta, var, w, x, y): \n","    ls = []\n","    for i in range(len(x)):\n","        ls.append((y[i] - w * x[i] + beta)**2)\n","    res = sum(ls)\n","    res = (1 / var) * res\n","    res = (1 / ( 2 * var )) * (res - len(x))\n","    return res\n","\n","x = np.array([8, 16, 22, 33, 50, 51])\n","y = np.array([5, 20, 14, 32, 42, 58])\n","w = [1, 1, 1] \n","\n","for i in range(1000):\n","    gradient = np.array([-log_likelihood_w(w[0], w[1],w[2], x,y), -log_likelihood_beta(w[0], w[1],w[2], x,y), -log_likelihood_var(w[0], w[1],w[2], x,y)])\n","    w = w - 0.01 * gradient\n","\n","print(w)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":2}
